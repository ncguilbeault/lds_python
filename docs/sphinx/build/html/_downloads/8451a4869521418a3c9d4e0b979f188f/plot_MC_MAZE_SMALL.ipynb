{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Learning and inference of latents with MC_MAZE_SMALL data\n\nThe code below learns and infers latents with MC_MAZE_SMALL data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport plotly.graph_objects as go\n\nfrom dandi.dandiapi import DandiAPIClient\nfrom pynwb import NWBHDF5IO\n\nimport ssm.inference\nimport ssm.learning\nimport ssm.neural_latents.utils\nimport ssm.neural_latents.plotting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define variables\nHere I am setting a small value to variable ``max_iter``, in order to build\nthis documentation quickly. You may want to set this variable to\n``max_iter=2000`` in combination with a small error tolerance ``tol=1e-3``.\n\nIn addition, you may want to plot all the data, and not just a small time\ninterval between ``from_time = 100.0`` and ``to_time = 130.0``, by setting\n``from_time = -np.inf`` and ``to_time = np.inf``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# data\nget_data_from_Dandi = True\ndandiset_ID = \"000140\"\ndandi_filepath = \"sub-Jenkins/sub-Jenkins_ses-small_desc-train_behavior+ecephys.nwb\"\nlocal_filepath = f\"../../../../projects/lds_neuralLatents_MC_MAZE_SMALL/data/{dandiset_ID}/sub-Jenkins/sub-Jenkins_ses-small_desc-train_behavior+ecephys.nwb\"\nbin_size = 0.02\n\n# plot\nevents_names = [\"start_time\", \"target_on_time\", \"go_cue_time\",\n                \"move_onset_time\", \"stop_time\"]\nevents_linetypes = [\"dot\", \"dash\", \"dashdot\", \"longdash\", \"solid\"]\nevents_colors_spikes = [\"white\", \"white\", \"white\", \"white\", \"white\"]\nevents_colors_latents = [\"black\", \"black\", \"black\", \"black\", \"black\"]\ncb_alpha = 0.3\nfrom_time = 100.0\nto_time = 130.0\n\n# model\nn_latents = 10\n\n# estimation initial conditions\nsigma_B = 0.1\nsigma_Z = 0.1\nsigma_Q = 0.1\nsigma_R = 0.1\nsigma_m0 = 0.1\nsigma_V0 = 0.1\n\n# estimation parameters\n# max_iter = 2000\nmax_iter = 2000\ntol = 1e-1\nvars_to_estimate = {\"B\": True, \"Q\": True, \"Z\": True, \"R\": True,\n                    \"m0\": True, \"V0\": True, }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if get_data_from_Dandi:\n    with DandiAPIClient() as client:\n        asset = client.get_dandiset(dandiset_ID,\n                                    \"draft\").get_asset_by_path(dandi_filepath)\n        s3_path = asset.get_content_url(follow_redirects=1, strip_query=True)\n        io = NWBHDF5IO(s3_path, mode=\"r\", driver=\"ros3\")\n        nwbfile = io.read()\n        units_df = nwbfile.units.to_dataframe()\n        trials_df = nwbfile.intervals[\"trials\"].to_dataframe()\nelse:\n    with NWBHDF5IO(local_filepath, 'r') as io:\n        nwbfile = io.read()\n        units_df = nwbfile.units.to_dataframe()\n        trials_df = nwbfile.intervals[\"trials\"].to_dataframe()\n\n\n# n_clusters\nn_clusters = units_df.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bin spikes\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# continuous spikes times\ncontinuous_spikes_times = [None for n in range(n_clusters)]\nfor n in range(n_clusters):\n    continuous_spikes_times[n] = units_df.iloc[n]['spike_times']\n\nbinned_spikes, bin_edges = ssm.neural_latents.utils.bin_spike_times(\n    spike_times=continuous_spikes_times, bin_size=bin_size)\nbin_centers = (bin_edges[1:] + bin_edges[:-1])/2\ntransformed_binned_spikes = np.sqrt(binned_spikes + 0.5)\n\n# clip data to plot\nfirst_index = np.where(bin_centers >= from_time)[0][0]\nlast_index = np.where(bin_centers <= to_time)[0][-1]\nto_plot_slice = slice(first_index, last_index)\nbin_centers_to_plot = bin_centers[to_plot_slice]\ntrials_df = trials_df[np.logical_and(\n    trials_df['start_time'] >= from_time,\n    trials_df['stop_time'] <= to_time,\n)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot binned spikes\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\ntrace = go.Heatmap(x=bin_centers_to_plot,\n                   z=transformed_binned_spikes[:, to_plot_slice],\n                   colorbar=dict(title=\"<b>Sqrt(spike_count+0.5)</b>\"))\nfig.add_trace(trace)\nssm.neural_latents.plotting.add_events_vlines(\n    fig=fig, trials_df=trials_df, events_names=events_names,\n    events_linetypes=events_linetypes, events_colors=events_colors_spikes)\nfig.update_xaxes(title=\"Time (sec)\")\nfig.update_yaxes(title=\"Cluster Index\")\nfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameter learning using expectation maximization\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "B0 = np.diag(np.random.normal(loc=0, scale=sigma_B, size=n_latents))\nZ0 = np.random.normal(loc=0, scale=sigma_Z, size=(n_clusters, n_latents))\nQ0 = np.diag(np.abs(np.random.normal(loc=0, scale=sigma_Q, size=n_latents)))\nR0 = np.diag(np.abs(np.random.normal(loc=0, scale=sigma_R, size=n_clusters)))\nm0_0 = np.random.normal(loc=0, scale=sigma_m0, size=n_latents)\nV0_0 = np.diag(np.abs(np.random.normal(loc=0, scale=sigma_V0, size=n_latents)))\n\noptim_res = ssm.learning.em_SS_LDS(\n    y=transformed_binned_spikes, B0=B0, Q0=Q0, Z0=Z0, R0=R0,\n    m0_0=m0_0, V0_0=V0_0, max_iter=max_iter, tol=tol,\n    vars_to_estimate=vars_to_estimate,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot log likelihood vs iteration number\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "N = len(optim_res[\"log_like\"])\niter_no = np.arange(0, N)\nfig = go.Figure()\ntrace = go.Scatter(x=iter_no,\n                   y=optim_res[\"log_like\"],\n                   mode=\"lines+markers\")\nfig.add_trace(trace)\nfig.update_layout(xaxis=dict(title=\"Iteration Number\"),\n                  yaxis=dict(title=\"Lower Bound\"))\nfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kalman filtering\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "filter_res = ssm.inference.filterLDS_SS_withMissingValues_np(\n    y=transformed_binned_spikes, B=optim_res[\"B\"], Q=optim_res[\"Q\"],\n    m0=optim_res[\"m0\"], V0=optim_res[\"V0\"], Z=optim_res[\"Z\"], R=optim_res[\"R\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kalman smoothing\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "smoothing_res = ssm.inference.smoothLDS_SS(\n    B=optim_res[\"B\"], xnn=filter_res[\"xnn\"], Pnn=filter_res[\"Pnn\"],\n    xnn1=filter_res[\"xnn1\"], Pnn1=filter_res[\"Pnn1\"],\n    m0=optim_res[\"m0\"], V0=optim_res[\"V0\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot smoothed states\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "o_means_to_plot, o_covs_to_plot = ssm.neural_latents.utils.ortogonalizeMeansAndCovs(\n    means=smoothing_res[\"xnN\"][:, :, to_plot_slice],\n    covs=smoothing_res[\"PnN\"][:, :, to_plot_slice], Z=optim_res[\"Z\"])\n\nfig = ssm.neural_latents.plotting.plot_latents(\n    means=o_means_to_plot,\n    covs=o_covs_to_plot,\n    bin_centers=bin_centers_to_plot,\n    trials_df=trials_df,\n    events_names=events_names,\n    events_linetypes=events_linetypes,\n    events_colors=events_colors_latents,\n    cb_alpha=cb_alpha,\n    legend_pattern=\"smoothing_{:d}\",\n)\n\nfig"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}